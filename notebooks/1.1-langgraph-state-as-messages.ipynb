{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State as Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to keep a log of the conversation? Something quite common in LLM applications.\n",
    "How could we do that across a traditional langgraph graph?\n",
    "\n",
    "The answer is by defining a state that allows us to add messages as the graph gets executed!\n",
    "\n",
    "But wait, what are messages?\n",
    "\n",
    "## Messages\n",
    "\n",
    "[Messages](https://python.langchain.com/v0.2/docs/concepts/#messages), capture different roles within a conversation. \n",
    "\n",
    "LangChain supports various message types, including \n",
    "\n",
    "1. `HumanMessage` - message from the user \n",
    "2. `AIMessage` - message from the chat model\n",
    "3. `SystemMessage` - message that instructs the behavior of the chat model\n",
    "4. `ToolMessage` - message from a tool call (calling a tool that performs some action) \n",
    "\n",
    "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call. \n",
    "\n",
    "Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, a message author \n",
    "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So, you work in AI engineering?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lucas\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lucas\n",
      "\n",
      "I want to learn about fine tunning local LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So, you work in AI engineering?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lucas\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about fine tunning local LLMs.\", name=\"Lucas\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, messages store the exchanges between model and user throughout a conversation. We can have them as keys in a state for a graph\n",
    "so that we can keep a log of the interactions throughout the graph and so that the chat models within a graph can have the context information\n",
    "for what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using messages as state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about what do we need to have these messages as parts of a graph state?\n",
    "We need:\n",
    "1. Something that stores the messages (like a list)\n",
    "2. Something that allows us to add more messages into this list as the graph is executed\n",
    "3. Something that can log metadata about the objects being added to the conversation so we can inspect them during debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='message 2', additional_kwargs={}, response_metadata={}, name='Lucas')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def my_node(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 1\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "def my_node2(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 2\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the node\n",
    "workflow.add_node(\"test_node\", my_node)\n",
    "workflow.add_node(\"test_node2\", my_node2)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"test_node\")\n",
    "\n",
    "# Set the exit point\n",
    "workflow.add_edge(\"test_node\", \"test_node2\")\n",
    "workflow.add_edge(\"test_node2\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the graph with initial state\n",
    "result = graph.invoke({\n",
    "    \"messages\": messages  # Using messages defined above\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait! Why do we only have one message in the final output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducers\n",
    "\n",
    "In langgraph the default is to [override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
    " \n",
    "However what we want is to append messages to our `messages` state key throughout the graph's execution.\n",
    " \n",
    "For that we use something called [`reducer` functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers), which allow us to specify\n",
    "how the states get updated. To do that we annotate the `messages` key with the `add_messages` reducer function as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we take the same graph as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='message 1', additional_kwargs={}, response_metadata={}, name='Lucas', id='460c0b1a-f4dd-49c4-9fa5-9a6c4fa25a1f'),\n",
       "  HumanMessage(content='message 2', additional_kwargs={}, response_metadata={}, name='Lucas', id='868c99bb-e436-472b-b192-70381a1eff63')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_node(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 1\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "def my_node2(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 2\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the node\n",
    "workflow.add_node(\"test_node\", my_node)\n",
    "workflow.add_node(\"test_node2\", my_node2)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"test_node\")\n",
    "\n",
    "# Set the exit point\n",
    "workflow.add_edge(\"test_node\", \"test_node2\")\n",
    "workflow.add_edge(\"test_node2\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the graph with initial state\n",
    "result = graph.invoke({\n",
    "    \"messages\": []  # Using messages defined above\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that now both messages are stored inside after the graph's execution.\n",
    "\n",
    "LangGraph allows us to simplify this a bit so we don't have to be writing this overcomplicated-looking class everytime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see a simple example actually using LLMs + messages as state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Tell me a short joke', additional_kwargs={}, response_metadata={}, id='899812f2-251e-492f-a50b-e10ed22d5ceb'),\n",
       "  AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 12, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-91b5dac4-d13e-4363-bcc5-cc1995c0f74d-0', usage_metadata={'input_tokens': 12, 'output_tokens': 18, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Define a node that uses the LLM to respond\n",
    "def chat_node(state):\n",
    "    # Get messages from state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Add response to messages\n",
    "    new_messages = messages + [response]\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the chat node\n",
    "workflow.add_node(\"chat\", chat_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"chat\")\n",
    "\n",
    "# Add edge to end\n",
    "workflow.add_edge(\"chat\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the graph with an initial message\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Tell me a short joke\")]\n",
    "})\n",
    "\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langgraph",
   "language": "python",
   "name": "oreilly-langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
